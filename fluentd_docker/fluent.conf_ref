<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

<match docker.*>
    @type record_modifier
    tag docker.after.process
</match>
<filter docker.after.process>
  @type record_transformer
  enable_ruby
  <record>
    "type" "log_container"
    "data" ${require 'json';require "open-uri";uri = 'http://169.254.169.250/2015-12-19/containers/';html_response = nil ;uuid=record["container_id"];env_id = "env_id123";open(uri,"Accept" => "application/json")  do |http| html_response = http.read end;container_array = JSON.parse(html_response);container_array.each do |info| if info['external_id']=='dc4a6e148c2daf322523aafc8827a34eebe2e7f4b3481074044628d9bb14296d';uuid=info['uuid'];env_id=info['labels']['io.rancher.container.name'];break;end; end; puts uuid;puts env_id;log_info = Hash["log_time" => time.strftime('%Y-%m-%dT%H:%M:%S.%L%:z'), "source"=>record["source"],"message"=>record["log"]];json_data = Hash["container_uuid"=>uuid,"environment_id"=>env_id,"timestamp"=>Time.now.strftime('%Y-%m-%dT%H:%M:%S.%L%:z'), "log_info"=>log_info];json_data.to_json}
  </record>
</filter> 

#in-place to avoid escape character
<filter docker.after.process>
  @type record_transformer
  renew_record true
  keep_keys type,data
</filter>
<match docker.after.process>
    type burrow
    key_name data
   action inplace
   format json
   tag docker.after.process.proc
</match>

<match docker.after.process.proc>
 @type copy
 <store>
  #@type stdout
  @type elasticsearch
  host 223.202.32.60
  port 8056
  logstash_format true
  logstash_prefix fluentd_from_container_to_es.log
  flush_interval 5s
  num_threads 2
</store>
<store>
  @type kafka
  brokers 223.202.32.59:8065 
  default_topic fluentd_from_container_to_kafka
</store>
</match>
